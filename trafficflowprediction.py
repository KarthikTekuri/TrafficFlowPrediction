# -*- coding: utf-8 -*-
"""TrafficFlowPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10zY7-1O2gYEE2CH6TjGeZXREL0W-HkF3

**Data Preprocessing**
"""

import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split

# Load the dataset
data = pd.read_csv('synthetic_traffic_data_with_congestion.csv')

# Step 1: Handle missing values (drop or fill)
data_cleaned = data.dropna()

# Step 2: One-hot encode categorical features
encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
encoded_features = encoder.fit_transform(data_cleaned[['Road_Condition', 'Traffic_Congestion']])

# Step 3: Date-time processing
data_cleaned['Time'] = pd.to_datetime(data_cleaned['Time'])
data_cleaned['hour'] = data_cleaned['Time'].dt.hour
data_cleaned['day_of_week'] = data_cleaned['Time'].dt.dayofweek

# Step 4: Normalize numerical features
scaler = StandardScaler()
numerical_features = ['Traffic_Flow', 'Average_Speed', 'Occupancy', 'Temperature',
                      'Humidity', 'Wind_Speed', 'Precipitation', 'Visibility']
scaled_data = scaler.fit_transform(data_cleaned[numerical_features])

# Step 5: Ensure target 'Traffic_Congestion' is encoded correctly
label_encoder = LabelEncoder()  # Convert categorical target to binary 0 and 1
data_cleaned['Traffic_Congestion'] = label_encoder.fit_transform(data_cleaned['Traffic_Congestion'])

# Step 6: Combine processed features
X = pd.concat([pd.DataFrame(scaled_data, columns=numerical_features),
               pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out())], axis=1)

# Step 7: Define the target variable
y = data_cleaned['Traffic_Congestion']

# Step 8: Split data into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**AutoEncoder**"""

import tensorflow as tf
from tensorflow.keras import layers, models

# Autoencoder model
input_dim = X_train.shape[1]

autoencoder = models.Sequential([
    layers.Dense(64, activation='relu', input_dim=input_dim),
    layers.Dense(32, activation='relu'),
    layers.Dense(64, activation='relu'),
    layers.Dense(input_dim, activation='sigmoid')  # Output layer
])

autoencoder.compile(optimizer='adam', loss='mse')

# Train autoencoder
autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_test, X_test))

"""**Neural Network**"""

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers

# Define the model architecture
def build_model(input_dim):
    model = models.Sequential([
        layers.Dense(64, activation='relu', input_dim=input_dim),
        layers.BatchNormalization(),
        layers.Dense(32, activation='relu'),
        layers.BatchNormalization(),
        layers.Dense(1, activation='sigmoid')  # Binary classification
    ])
    return model

# Get the input dimension based on the preprocessed dataset
input_dim = X_train.shape[1]  # Number of features after one-hot encoding and scaling

# Define a function to compile and train the model with different gradient descent methods
def train_model(optimizer, batch_size):
    # Build and compile the model
    model = build_model(input_dim=input_dim)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    # Train the model
    history = model.fit(X_train, y_train, epochs=50, batch_size=batch_size, validation_data=(X_test, y_test))

    # Return training history for evaluation and plotting
    return history

# Full Batch Gradient Descent (batch size = entire dataset)
full_batch_size = X_train.shape[0]  # Full dataset size
adam_full_batch = optimizers.Adam()
adagrad_full_batch = optimizers.Adagrad()

print("Training with Full Batch Gradient Descent (Adam Optimizer):")
history_full_batch_adam = train_model(adam_full_batch, full_batch_size)

print("Training with Full Batch Gradient Descent (Adagrad Optimizer):")
history_full_batch_adagrad = train_model(adagrad_full_batch, full_batch_size)

# Mini-Batch Gradient Descent (batch size = 32)
mini_batch_size = 32
adam_mini_batch = optimizers.Adam()
adagrad_mini_batch = optimizers.Adagrad()

print("Training with Mini-Batch Gradient Descent (Adam Optimizer):")
history_mini_batch_adam = train_model(adam_mini_batch, mini_batch_size)

print("Training with Mini-Batch Gradient Descent (Adagrad Optimizer):")
history_mini_batch_adagrad = train_model(adagrad_mini_batch, mini_batch_size)

# Stochastic Gradient Descent (batch size = 1)
stochastic_batch_size = 1
adam_sgd = optimizers.Adam()
adagrad_sgd = optimizers.Adagrad()

print("Training with Stochastic Gradient Descent (Adam Optimizer):")
history_sgd_adam = train_model(adam_sgd, stochastic_batch_size)

print("Training with Stochastic Gradient Descent (Adagrad Optimizer):")
history_sgd_adagrad = train_model(adagrad_sgd, stochastic_batch_size)

"""**Observations and Comparisions**"""

import matplotlib.pyplot as plt

def plot_history(histories, labels, metric='accuracy'):
    for history, label in zip(histories, labels):
        plt.plot(history.history[metric], label=label)
    plt.title(f'Model {metric} comparison')
    plt.ylabel(metric)
    plt.xlabel('Epoch')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
    plt.show()

# Compare accuracy for each method
plot_history(
    [history_full_batch_adam, history_full_batch_adagrad,
     history_mini_batch_adam, history_mini_batch_adagrad,
     history_sgd_adam, history_sgd_adagrad],
    ['Full Batch Adam', 'Full Batch Adagrad',
     'Mini Batch Adam', 'Mini Batch Adagrad',
     'SGD Adam', 'SGD Adagrad'],
    metric='accuracy'
)

# Compare loss for each method
plot_history(
    [history_full_batch_adam, history_full_batch_adagrad,
     history_mini_batch_adam, history_mini_batch_adagrad,
     history_sgd_adam, history_sgd_adagrad],
    ['Full Batch Adam', 'Full Batch Adagrad',
     'Mini Batch Adam', 'Mini Batch Adagrad',
     'SGD Adam', 'SGD Adagrad'],
    metric='loss'
)